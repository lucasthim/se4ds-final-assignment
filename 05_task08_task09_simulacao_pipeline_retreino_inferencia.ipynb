{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 08 - Simulação de um fluxo (pipeline) de treino/retreino automático\n",
    "\n",
    "Sub Tarefas\n",
    "- Utilizar todas as classes criadas para executar a esteira (pipeline) de treino.\n",
    "- Salvar os modelos otimizados.\n",
    "- Escolher um modelo final baseado no conjunto de testes.\n",
    "- Salvar o modelo final.\n",
    "\n",
    "Definição de Pronto:\n",
    "- Ter o código da esteira estruturado e rodando de ponta a ponta.\n",
    "- Ter um modelo final treinado e salvo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_access_handler import DataAccessHandler\n",
    "from src.feature_selector import FeatureSelector\n",
    "from src.model import Model,ModelSelector,ModelOptimizer\n",
    "from src.utils import f1_score_micro\n",
    "\n",
    "CV_SPLITS = 5\n",
    "OPTIMIZATION_TRIALS = 10\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = \"./data/\"\n",
    "MODEL_PATH = \"./models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data into memory...\n",
      "Data loaded!\n",
      "\n",
      "Selecting best features for training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucasthimoteo/projects/se4ds-final-assignment/notebooks/model.py:141: ExperimentalWarning: OptunaSearchCV is experimental (supported from v0.17.0). The interface can change in the future.\n",
      "  optimized_model = OptunaSearchCV(model_pipeline,\n",
      "\u001b[32m[I 2022-07-19 20:12:49,747]\u001b[0m A new study created in memory with name: no-name-b8be0f1b-34f7-4bf3-aef6-9ef5b25ba883\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:12:49,768]\u001b[0m Trial 0 finished with value: 0.7486666666666666 and parameters: {'lr__C': 0.00046025999465485514}. Best is trial 0 with value: 0.7486666666666666.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:12:49,789]\u001b[0m Trial 1 finished with value: 0.7506666666666667 and parameters: {'lr__C': 0.0006321850156886694}. Best is trial 1 with value: 0.7506666666666667.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:12:49,829]\u001b[0m Trial 2 finished with value: 0.834 and parameters: {'lr__C': 0.06796419128240948}. Best is trial 2 with value: 0.834.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:12:49,855]\u001b[0m Trial 3 finished with value: 0.8026666666666668 and parameters: {'lr__C': 0.013792757559148971}. Best is trial 2 with value: 0.834.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:12:49,876]\u001b[0m Trial 4 finished with value: 0.7513333333333333 and parameters: {'lr__C': 0.001044808325316867}. Best is trial 2 with value: 0.834.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:12:49,922]\u001b[0m Trial 5 finished with value: 0.836 and parameters: {'lr__C': 0.2644403261547257}. Best is trial 5 with value: 0.836.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atributos Selecionados: ['baseline value', 'accelerations', 'uterine_contractions', 'prolongued_decelerations', 'abnormal_short_term_variability', 'mean_value_of_short_term_variability', 'percentage_of_time_with_abnormal_long_term_variability', 'mean_value_of_long_term_variability', 'histogram_width', 'histogram_min', 'histogram_mode', 'histogram_mean', 'histogram_median']\n",
      "Best features selected!\n",
      "\n",
      "Optimizing available ML models...\n",
      "Finding best hyperparams for Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-19 20:12:49,946]\u001b[0m Trial 6 finished with value: 0.7813333333333333 and parameters: {'lr__C': 0.00614369123919829}. Best is trial 5 with value: 0.836.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:12:49,975]\u001b[0m Trial 7 finished with value: 0.8233333333333333 and parameters: {'lr__C': 0.027150716171105076}. Best is trial 5 with value: 0.836.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:12:49,995]\u001b[0m Trial 8 finished with value: 0.7466666666666667 and parameters: {'lr__C': 0.00016209190420362666}. Best is trial 5 with value: 0.836.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:12:50,016]\u001b[0m Trial 9 finished with value: 0.7513333333333334 and parameters: {'lr__C': 0.0005982409991499023}. Best is trial 5 with value: 0.836.\u001b[0m\n",
      "/Users/lucasthimoteo/projects/se4ds-final-assignment/notebooks/model.py:141: ExperimentalWarning: OptunaSearchCV is experimental (supported from v0.17.0). The interface can change in the future.\n",
      "  optimized_model = OptunaSearchCV(model_pipeline,\n",
      "\u001b[32m[I 2022-07-19 20:12:50,028]\u001b[0m A new study created in memory with name: no-name-48db8728-a763-4451-902a-e7b9f1e0dcf2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression optimized!\n",
      "\n",
      "Finding best hyperparams for Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-19 20:12:50,336]\u001b[0m Trial 0 finished with value: 0.9306666666666666 and parameters: {'rf__n_estimators': 58, 'rf__max_depth': 95, 'rf__min_samples_split': 6, 'rf__min_samples_leaf': 3}. Best is trial 0 with value: 0.9306666666666666.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:12:51,545]\u001b[0m Trial 1 finished with value: 0.8546666666666667 and parameters: {'rf__n_estimators': 296, 'rf__max_depth': 14, 'rf__min_samples_split': 17, 'rf__min_samples_leaf': 34}. Best is trial 0 with value: 0.9306666666666666.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:12:52,547]\u001b[0m Trial 2 finished with value: 0.8720000000000001 and parameters: {'rf__n_estimators': 238, 'rf__max_depth': 88, 'rf__min_samples_split': 18, 'rf__min_samples_leaf': 26}. Best is trial 0 with value: 0.9306666666666666.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:12:52,689]\u001b[0m Trial 3 finished with value: 0.8539999999999999 and parameters: {'rf__n_estimators': 31, 'rf__max_depth': 8, 'rf__min_samples_split': 20, 'rf__min_samples_leaf': 35}. Best is trial 0 with value: 0.9306666666666666.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:12:53,768]\u001b[0m Trial 4 finished with value: 0.882 and parameters: {'rf__n_estimators': 243, 'rf__max_depth': 70, 'rf__min_samples_split': 4, 'rf__min_samples_leaf': 16}. Best is trial 0 with value: 0.9306666666666666.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:12:54,310]\u001b[0m Trial 5 finished with value: 0.8966666666666667 and parameters: {'rf__n_estimators': 117, 'rf__max_depth': 85, 'rf__min_samples_split': 13, 'rf__min_samples_leaf': 11}. Best is trial 0 with value: 0.9306666666666666.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:12:55,533]\u001b[0m Trial 6 finished with value: 0.874 and parameters: {'rf__n_estimators': 288, 'rf__max_depth': 35, 'rf__min_samples_split': 11, 'rf__min_samples_leaf': 25}. Best is trial 0 with value: 0.9306666666666666.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:12:55,794]\u001b[0m Trial 7 finished with value: 0.8846666666666667 and parameters: {'rf__n_estimators': 56, 'rf__max_depth': 92, 'rf__min_samples_split': 15, 'rf__min_samples_leaf': 17}. Best is trial 0 with value: 0.9306666666666666.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:12:56,265]\u001b[0m Trial 8 finished with value: 0.8546666666666667 and parameters: {'rf__n_estimators': 111, 'rf__max_depth': 32, 'rf__min_samples_split': 2, 'rf__min_samples_leaf': 31}. Best is trial 0 with value: 0.9306666666666666.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:12:57,206]\u001b[0m Trial 9 finished with value: 0.8646666666666667 and parameters: {'rf__n_estimators': 224, 'rf__max_depth': 30, 'rf__min_samples_split': 20, 'rf__min_samples_leaf': 28}. Best is trial 0 with value: 0.9306666666666666.\u001b[0m\n",
      "/Users/lucasthimoteo/projects/se4ds-final-assignment/notebooks/model.py:141: ExperimentalWarning: OptunaSearchCV is experimental (supported from v0.17.0). The interface can change in the future.\n",
      "  optimized_model = OptunaSearchCV(model_pipeline,\n",
      "\u001b[32m[I 2022-07-19 20:12:57,275]\u001b[0m A new study created in memory with name: no-name-43d5159c-1443-48a3-b903-70e479115488\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest optimized!\n",
      "\n",
      "Finding best hyperparams for Light GBM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-19 20:13:00,778]\u001b[0m Trial 0 finished with value: 0.908 and parameters: {'lgbm__n_estimators': 24, 'lgbm__max_depth': 53, 'lgbm__learning_rate': 0.06259960211800354, 'lgbm__num_leaves': 29, 'lgbm__subsample_for_bin': 27402}. Best is trial 0 with value: 0.908.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:13:14,548]\u001b[0m Trial 1 finished with value: 0.884 and parameters: {'lgbm__n_estimators': 236, 'lgbm__max_depth': 78, 'lgbm__learning_rate': 0.05416708338246849, 'lgbm__num_leaves': 11, 'lgbm__subsample_for_bin': 12}. Best is trial 0 with value: 0.908.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:14:04,899]\u001b[0m Trial 2 finished with value: 0.9006666666666666 and parameters: {'lgbm__n_estimators': 373, 'lgbm__max_depth': 32, 'lgbm__learning_rate': 0.0577009012507725, 'lgbm__num_leaves': 26, 'lgbm__subsample_for_bin': 14}. Best is trial 0 with value: 0.908.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:14:29,984]\u001b[0m Trial 3 finished with value: 0.9366666666666668 and parameters: {'lgbm__n_estimators': 101, 'lgbm__max_depth': 37, 'lgbm__learning_rate': 0.05089806724955585, 'lgbm__num_leaves': 50, 'lgbm__subsample_for_bin': 2517}. Best is trial 3 with value: 0.9366666666666668.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:14:45,229]\u001b[0m Trial 4 finished with value: 0.898 and parameters: {'lgbm__n_estimators': 330, 'lgbm__max_depth': 32, 'lgbm__learning_rate': 0.2123750087016702, 'lgbm__num_leaves': 8, 'lgbm__subsample_for_bin': 12}. Best is trial 3 with value: 0.9366666666666668.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:15:06,687]\u001b[0m Trial 5 finished with value: 0.9353333333333333 and parameters: {'lgbm__n_estimators': 184, 'lgbm__max_depth': 40, 'lgbm__learning_rate': 0.08153487555002126, 'lgbm__num_leaves': 21, 'lgbm__subsample_for_bin': 139}. Best is trial 3 with value: 0.9366666666666668.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:15:22,666]\u001b[0m Trial 6 finished with value: 0.9413333333333332 and parameters: {'lgbm__n_estimators': 216, 'lgbm__max_depth': 21, 'lgbm__learning_rate': 0.18677426892874643, 'lgbm__num_leaves': 29, 'lgbm__subsample_for_bin': 1273}. Best is trial 6 with value: 0.9413333333333332.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:15:30,888]\u001b[0m Trial 7 finished with value: 0.9446666666666665 and parameters: {'lgbm__n_estimators': 164, 'lgbm__max_depth': 17, 'lgbm__learning_rate': 0.07930740205834103, 'lgbm__num_leaves': 9, 'lgbm__subsample_for_bin': 2722}. Best is trial 7 with value: 0.9446666666666665.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:15:54,933]\u001b[0m Trial 8 finished with value: 0.9480000000000001 and parameters: {'lgbm__n_estimators': 395, 'lgbm__max_depth': 84, 'lgbm__learning_rate': 0.1723066020017953, 'lgbm__num_leaves': 38, 'lgbm__subsample_for_bin': 525}. Best is trial 8 with value: 0.9480000000000001.\u001b[0m\n",
      "\u001b[32m[I 2022-07-19 20:15:59,239]\u001b[0m Trial 9 finished with value: 0.9446666666666665 and parameters: {'lgbm__n_estimators': 75, 'lgbm__max_depth': 93, 'lgbm__learning_rate': 0.4871780539871867, 'lgbm__num_leaves': 15, 'lgbm__subsample_for_bin': 427}. Best is trial 8 with value: 0.9480000000000001.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Light GBM optimized!\n",
      "All models optimized!\n",
      "\n",
      "All optimized models saved!\n",
      "\n",
      "Loading saved models into memory...\n",
      "All optimized models loaded into memory!\n",
      "\n",
      "Loading test set for model selection...\n",
      "Data loaded into memory!\n",
      "\n",
      "Choosing final model...\n",
      "logistic_regression final score: 0.7939\n",
      "random_forest final score: 0.9361\n",
      "light_gbm final score: 0.9537\n",
      "\n",
      "Best model is light_gbm with f1-score-micro =  0.9537 for the test set.\n",
      "Saving final model...\n",
      "Final model saved!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading training data into memory...\")\n",
    "access_handler = DataAccessHandler(main_path=DATA_PATH)\n",
    "df_train = access_handler.load(dataset_type=\"train\")\n",
    "print(\"Data loaded!\\n\")\n",
    "\n",
    "target = 'fetal_health'\n",
    "X,y = df_train.drop(columns=target),df_train[target].values.ravel()\n",
    "\n",
    "print(\"Selecting best features for training...\")\n",
    "feature_selector = FeatureSelector()\n",
    "feature_selector.select_best_features(X=X,y=y)\n",
    "feature_selector.save_best_features(path = DATA_PATH)\n",
    "features = feature_selector.get_selected_features\n",
    "print(\"Best features selected!\\n\")\n",
    "\n",
    "X,y = df_train[features],df_train[target].values.ravel()\n",
    "\n",
    "print(\"Optimizing available ML models...\")\n",
    "model_optimizer = ModelOptimizer(random_state = RANDOM_STATE, optimization_trials = OPTIMIZATION_TRIALS, cv_splits = CV_SPLITS)\n",
    "model_optimizer.optimize_all_models(X,y)\n",
    "lr_best,rf_best,lgbm_best = model_optimizer.get_optimized_models\n",
    "print(\"All models optimized!\\n\")\n",
    "\n",
    "lr_model = Model(model = lr_best)\n",
    "lr_model.save(path=MODEL_PATH,model_name=\"logistic_regression\")\n",
    "\n",
    "rf_model = Model(model = rf_best)\n",
    "rf_model.save(path=MODEL_PATH,model_name=\"random_forest\")\n",
    "\n",
    "lgbm_model = Model(model = lgbm_best)\n",
    "lgbm_model.save(path=MODEL_PATH,model_name=\"light_gbm\")\n",
    "\n",
    "del lr_best,rf_best,lgbm_best,lr_model,rf_model,lgbm_model\n",
    "print(\"All optimized models saved!\\n\")\n",
    "\n",
    "print(\"Loading saved models into memory...\")\n",
    "lr_model = Model()\n",
    "lr_model.load(path=MODEL_PATH,model_name=\"logistic_regression\")\n",
    "\n",
    "rf_model = Model()\n",
    "rf_model.load(path=MODEL_PATH,model_name=\"random_forest\")\n",
    "\n",
    "lgbm_model = Model()\n",
    "lgbm_model.load(path=MODEL_PATH,model_name=\"light_gbm\")\n",
    "print(\"All optimized models loaded into memory!\\n\")\n",
    "\n",
    "\n",
    "print(\"Loading test set for model selection...\")\n",
    "df_test = access_handler.load(dataset_type=\"test\")\n",
    "X,y = df_test[features],df_test[target].values.ravel()\n",
    "print(\"Data loaded into memory!\\n\")\n",
    "\n",
    "print(\"Choosing final model...\")\n",
    "model_selector = ModelSelector(models=[lr_model,rf_model,lgbm_model],\n",
    "                            model_names=[\"logistic_regression\",\"random_forest\",\"light_gbm\"])\n",
    "model_selector.select_best_model(X,y)\n",
    "\n",
    "print(\"Saving final model...\")\n",
    "model_selector.get_winner_model.save(path = MODEL_PATH,model_name = \"winner_model\")\n",
    "print(\"Final model saved!\")\n",
    "\n",
    "del model_selector,df_test,access_handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusões:\n",
    "\n",
    "- Devido ao tempo mais elevado de treino do LightGBM, os modelos foram otimizados por apenas 10 rodadas cada um.\n",
    "- A esteira de treinamento e escolha do melhor modelo construída com as classes pode ser executada de ponta a ponta, com bastante facilidade e clareza de todos os passos.\n",
    "- O melhor modelo escolhido ainda segue o código experimental das task 06 e 07. Isso porque a seed usada é a mesma. Caso a seed seja modificada, o resultado final poderá ser diferente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 09 - Simulação de um Fluxo de Inferência\n",
    "\n",
    "Sub Tarefas:\n",
    "- Utilizar todas as classes criadas para executar a esteira (pipeline) de treino.\n",
    "- Fazer uma inferência para uma amostra dos dados de testes\n",
    "\n",
    "Definição de Pronto:\n",
    "- Ter a inferência execudada de ponta a ponta para uma amostra dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_access_handler import DataAccessHandler\n",
    "from src.feature_selector import FeatureSelector\n",
    "from src.model import Model\n",
    "\n",
    "DATA_PATH = \"./data/\"\n",
    "MODEL_PATH = \"./models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data into memory...\n",
      "Data loaded!\n",
      "\n",
      "Best features selected!\n",
      "\n",
      "Model loaded into memory!\n",
      "\n",
      "Prediction for provided sample:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 3., 1., 3., 2., 2.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading training data into memory...\")\n",
    "access_handler = DataAccessHandler(main_path=DATA_PATH)\n",
    "df = access_handler.load(dataset_type=\"test\").iloc[:10] # simulando 10 amostras para inferência\n",
    "print(\"Data loaded!\\n\")\n",
    "\n",
    "feature_selector = FeatureSelector()\n",
    "feature_selector.load_best_features(path = DATA_PATH)\n",
    "features = feature_selector.get_selected_features\n",
    "print(\"Best features selected!\\n\")\n",
    "\n",
    "target = 'fetal_health'\n",
    "X = df[features]\n",
    "\n",
    "print(\"Model loaded into memory!\\n\")\n",
    "model = Model()\n",
    "model.load(path=MODEL_PATH,model_name=\"winner_model\")\n",
    "\n",
    "print(\"Prediction for provided sample:\")\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusões:\n",
    "\n",
    "- A esteira de inferência construída com as classes pode ser executada de ponta a ponta com bastante facilidade e clareza de todos os passos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('base_ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff5b71a57004aac41c92d4008d66e8c8a20573ad91c6a09451338db6162a1ae6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
